{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CbpiHEn0XVjr"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwCyw33kYQ-c",
        "outputId": "0c0650d5-49da-4cfa-cb1f-c1ad5ecf1eb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_data = \"I am Satyam Jhawar, a student of Christ University - Central Campus, Bengaluru who is pursuing MSc in AI and ML ğŸ˜\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "print (nltk_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ufLUSGX6FS",
        "outputId": "09083dce-7366-47a0-935f-71d32ee2bdb8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'Satyam', 'Jhawar', ',', 'a', 'student', 'of', 'Christ', 'University', '-', 'Central', 'Campus', ',', 'Bengaluru', 'who', 'is', 'pursuing', 'MSc', 'in', 'AI', 'and', 'ML', 'ğŸ˜']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word tokenization is the process of splitting a text into individual words.\n",
        "\n",
        "Word tokenization can be straightforward for languages with spaces between words but can be more complex for languages like Chinese or Thai that don't use spaces between words."
      ],
      "metadata": {
        "id": "N-_XX_-QGTGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_data = \"I am Satyam Jhawar. I am a student of Christ University - Central Campus, Bengaluru, Karnataka. I am pursuing MSc in AI and ML ğŸ–¥ğŸ–¥ğŸ–¥\"\n",
        "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
        "print (nltk_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF4o1_N5YLzu",
        "outputId": "f4c2ff35-4ec4-415b-cd04-86f39ea1086a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I am Satyam Jhawar.', 'I am a student of Christ University - Central Campus, Bengaluru, Karnataka.', 'I am pursuing MSc in AI and ML ğŸ–¥ğŸ–¥ğŸ–¥']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence tokenization involves splitting a text into individual sentences.\n",
        "Essential for tasks like text summarization, machine translation, and document clustering."
      ],
      "metadata": {
        "id": "hPLvrmaEGt39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "metadata": {
        "id": "ZzRHYYeAY4cQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am Satyam Jhawar, a Student of Christ University - Central campus, Bengaluru. I am Cricket Freak and have been playing this sport since the last 13 years and definetly counting. ğŸğŸğŸâ¤â¤\"\n",
        "print(\"\\nOriginal string:\")\n",
        "print(text)\n",
        "result = WordPunctTokenizer().tokenize(text)\n",
        "print(\"\\nSplit all punctuation into separate tokens:\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VHgKkdVYoQn",
        "outputId": "24e6001e-660f-4813-d633-136aa4d3b9a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original string:\n",
            "I am Satyam Jhawar, a Student of Christ University - Central campus, Bengaluru. I am Cricket Freak and have been playing this sport since the last 13 years and definetly counting. ğŸğŸğŸâ¤â¤\n",
            "\n",
            "Split all punctuation into separate tokens:\n",
            "['I', 'am', 'Satyam', 'Jhawar', ',', 'a', 'Student', 'of', 'Christ', 'University', '-', 'Central', 'campus', ',', 'Bengaluru', '.', 'I', 'am', 'Cricket', 'Freak', 'and', 'have', 'been', 'playing', 'this', 'sport', 'since', 'the', 'last', '13', 'years', 'and', 'definetly', 'counting', '.', 'ğŸğŸğŸâ¤â¤']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation-based Tokenizer splits text based on punctuation marks such as periods, commas, and semicolons.\n",
        "It is suitable for simple text processing tasks where accuracy is not critical."
      ],
      "metadata": {
        "id": "Y-UfxISDG6ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer"
      ],
      "metadata": {
        "id": "Ofhwddj-Y7x6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = '''I am Satyam Jhawar, a student of Christ University - Central Campus, Bengaluru who is pursuing MSc in AI and ML ğŸ˜. I am Cricket Freak and have been playing this sport since the last 13 years and definetly counting. ğŸğŸğŸâ¤â¤'''\n",
        "d = TreebankWordDetokenizer()\n",
        "t = TreebankWordTokenizer()\n",
        "toks = t.tokenize(s)\n",
        "d.detokenize(toks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "TnRa7sbfZKvf",
        "outputId": "63886353-3ba0-483a-f378-a4c94c4b3554"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am Satyam Jhawar, a student of Christ University - Central Campus, Bengaluru who is pursuing MSc in AI and ML ğŸ˜. I am Cricket Freak and have been playing this sport since the last 13 years and definetly counting. ğŸğŸğŸâ¤â¤'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Treebank Word Tokenizer is based on the Penn Treebank corpus and uses regular expressions to tokenize text according to conventions used in the corpus.\n",
        "Beneficial for tasks requiring fine-grained tokenization, such as syntactic parsing and part-of-speech tagging."
      ],
      "metadata": {
        "id": "VLLIUWDzHreb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "e2aL8Vs3ZiM_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk = TweetTokenizer()\n",
        "x = \"Satyam is a very handsome boy. ğŸ¤“ğŸ¤“ğŸ¤“ğŸ¤“\"\n",
        "y = tk.tokenize(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-lsq-iBZ3DO",
        "outputId": "b1a13d95-c003-4b13-af46-bc78659acc0f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Satyam', 'is', 'a', 'very', 'handsome', 'boy', '.', 'ğŸ¤“', 'ğŸ¤“', 'ğŸ¤“']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweet Tokenizer is designed specifically for tokenizing tweets, which often contain unique characteristics such as hashtags, mentions, and emojis.\n",
        "Useful for sentiment analysis, trend analysis, and social media monitoring."
      ],
      "metadata": {
        "id": "BsWAeUhIIQcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer"
      ],
      "metadata": {
        "id": "JeQZA2vVIoob"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am Satyam Jhawar, a Student of Christ University - Central campus, Bengaluru. I am Cricket Freak and have been playing this sport since the last 13 years and definetly counting. ğŸğŸğŸâ¤â¤\""
      ],
      "metadata": {
        "id": "sS3OdiPNI6Bv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MWETokenizer().add_mwe(('Satyam Jhawar','Christ University'))\n",
        "MWETokenizer().tokenize(nltk.word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-T0elLDIuXu",
        "outputId": "fccc30cb-32ee-4550-dfa5-503fad97681c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'am',\n",
              " 'Satyam',\n",
              " 'Jhawar',\n",
              " ',',\n",
              " 'a',\n",
              " 'Student',\n",
              " 'of',\n",
              " 'Christ',\n",
              " 'University',\n",
              " '-',\n",
              " 'Central',\n",
              " 'campus',\n",
              " ',',\n",
              " 'Bengaluru',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'Cricket',\n",
              " 'Freak',\n",
              " 'and',\n",
              " 'have',\n",
              " 'been',\n",
              " 'playing',\n",
              " 'this',\n",
              " 'sport',\n",
              " 'since',\n",
              " 'the',\n",
              " 'last',\n",
              " '13',\n",
              " 'years',\n",
              " 'and',\n",
              " 'definetly',\n",
              " 'counting',\n",
              " '.',\n",
              " 'ğŸğŸğŸâ¤â¤']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Word Expression Tokenizer identifies and tokenizes multi-word expressions (MWEs), such as \"New York\" or \"machine learning,\" as single units.\n"
      ],
      "metadata": {
        "id": "TqoYvRHIJZJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "rQ8qGAwFaBpZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = (\"Natural language processing (NLP) is a field \" +\n",
        "       \"of computer science, artificial intelligence \" +\n",
        "       \"and computational linguistics concerned with \" +\n",
        "       \"the interactions between computers and human \" +\n",
        "       \"(natural) languages ğŸ–¥ğŸ–¥ğŸ–¥, and, in particular, \" +\n",
        "       \"concerned with programming computers to \" +\n",
        "       \"fruitfully process large natural language \" +\n",
        "       \"corpora. ğŸ–¥ğŸ–¥ğŸ–¥ Challenges in natural language \" +\n",
        "       \"processing frequently involve natural \" +\n",
        "       \"language understanding, natural language\" +\n",
        "       \"generation frequently from formal, machine\" +\n",
        "       \"-readable logical forms), connecting language \" +\n",
        "       \"and machine perception, managing human-\" +\n",
        "       \"computer dialog systems, or some combination \" +\n",
        "       \"thereof. ğŸ–¥ğŸ–¥ğŸ–¥\")\n",
        "\n",
        "blob_object = TextBlob(text)\n",
        "\n",
        "print(\" Word Tokenize :\\n\", blob_object.words)\n",
        "\n",
        "print(\"\\n Sentence Tokenize :\\n\", blob_object.sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd8ib06-TjbA",
        "outputId": "9fcadee8-0697-403f-cdfb-a8e9aa7d900b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Word Tokenize :\n",
            " ['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'field', 'of', 'computer', 'science', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'ğŸ–¥ğŸ–¥ğŸ–¥', 'and', 'in', 'particular', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'ğŸ–¥ğŸ–¥ğŸ–¥', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'and', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'or', 'some', 'combination', 'thereof', 'ğŸ–¥ğŸ–¥ğŸ–¥']\n",
            "\n",
            " Sentence Tokenize :\n",
            " [Sentence(\"Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages ğŸ–¥ğŸ–¥ğŸ–¥, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.\"), Sentence(\"ğŸ–¥ğŸ–¥ğŸ–¥ Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\"), Sentence(\"ğŸ–¥ğŸ–¥ğŸ–¥\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob library tokenizer splits text into wordsusing the Word class's tokenization method.\n",
        "Suitable for rapid prototyping, educational purposes, and simple NLP tasks."
      ],
      "metadata": {
        "id": "7YwJ8diZJbAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "ffINFofATwEU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"Satyam studies in Christ University. He is 23 years old. ğŸ”ğŸ”ğŸ”\")\n",
        "for token in doc:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AEjiowbT37U",
        "outputId": "82d1dabc-fb37-4523-8712-8fa4f5b78cbb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Satyam\n",
            "studies\n",
            "in\n",
            "Christ\n",
            "University\n",
            ".\n",
            "He\n",
            "is\n",
            "23\n",
            "years\n",
            "old\n",
            ".\n",
            "ğŸ”\n",
            "ğŸ”\n",
            "ğŸ”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy's tokenizer is highly customizable and designed for efficiency and accuracy.\n",
        "Ideal for various NLP tasks, including named entity recognition, dependency parsing, and text classification."
      ],
      "metadata": {
        "id": "0Ff-MKZdKXI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize"
      ],
      "metadata": {
        "id": "i9sD9VixT_vZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Welcome to Christ University. ğŸ¤“ğŸ¤“ğŸ¤“ğŸ¤“\"\n",
        "tokens = list(tokenize(text))\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "Fo83ue1mUPHN",
        "outputId": "d56e5e33-4d60-464f-e75d-ee6cdd905392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Welcome', 'to', 'Christ', 'University']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim offers a simple word tokenizer as part of its toolkit for topic modeling and natural language processing.\n",
        "Well-suited for tasks like topic modeling, document similarity analysis, and word embedding generation."
      ],
      "metadata": {
        "id": "9eLh3DM9K4tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "Vkr_PEM9UQaY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am Satyam Jhawar, a Student of Christ University - Central campus, Bengaluru. I am Cricket Freak and have been playing this sport since the last 13 years and definetly counting. ğŸğŸğŸâ¤â¤\"\n"
      ],
      "metadata": {
        "id": "m6cc1z7LLLwd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "MGR4KAT9LNR3",
        "outputId": "1bc80c1a-1115-40ad-d02f-d136cc528185",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 1,\n",
              " 'e': 2,\n",
              " 't': 3,\n",
              " 'n': 4,\n",
              " 'i': 5,\n",
              " 's': 6,\n",
              " 'r': 7,\n",
              " 'c': 8,\n",
              " 'u': 9,\n",
              " 'y': 10,\n",
              " 'h': 11,\n",
              " 'l': 12,\n",
              " 'm': 13,\n",
              " 'd': 14,\n",
              " 'o': 15,\n",
              " 'f': 16,\n",
              " 'p': 17,\n",
              " 'g': 18,\n",
              " 'ğŸ': 19,\n",
              " 'v': 20,\n",
              " 'b': 21,\n",
              " 'k': 22,\n",
              " 'â¤': 23,\n",
              " 'j': 24,\n",
              " 'w': 25,\n",
              " '1': 26,\n",
              " '3': 27}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras, a deep learning library, provides tokenization utilities for preparing text data for neural network models.\n",
        "Essential for building deep learning models for tasks like text classification, sequence generation, and language modeling."
      ],
      "metadata": {
        "id": "EFMBg0E0LBIt"
      }
    }
  ]
}